{
  "affected": [
    {
      "database_specific": {
        "source": "https://storage.googleapis.com/cve-osv-conversion/osv-output/CVE-2025-38242.json"
      },
      "ecosystem_specific": {
        "urgency": "not yet assigned"
      },
      "package": {
        "ecosystem": "Debian:13",
        "name": "linux",
        "purl": "pkg:deb/debian/linux?arch=source"
      },
      "ranges": [
        {
          "events": [
            {
              "introduced": "0"
            }
          ],
          "type": "ECOSYSTEM"
        }
      ],
      "versions": [
        "6.1.106-1",
        "6.1.106-2",
        "6.1.106-3",
        "6.1.112-1",
        "6.1.115-1",
        "6.1.119-1",
        "6.1.123-1",
        "6.1.124-1",
        "6.1.128-1",
        "6.1.129-1",
        "6.1.133-1",
        "6.1.135-1",
        "6.1.137-1",
        "6.1.139-1",
        "6.1.140-1",
        "6.1.27-1",
        "6.1.37-1",
        "6.1.38-1",
        "6.1.38-2",
        "6.1.38-2~bpo11+1",
        "6.1.38-3",
        "6.1.38-4",
        "6.1.38-4~bpo11+1",
        "6.1.52-1",
        "6.1.55-1",
        "6.1.55-1~bpo11+1",
        "6.1.64-1",
        "6.1.66-1",
        "6.1.67-1",
        "6.1.69-1",
        "6.1.69-1~bpo11+1",
        "6.1.76-1",
        "6.1.76-1~bpo11+1",
        "6.1.82-1",
        "6.1.85-1",
        "6.1.90-1",
        "6.1.90-1~bpo11+1",
        "6.1.94-1",
        "6.1.94-1~bpo11+1",
        "6.1.98-1",
        "6.1.99-1",
        "6.10-1~exp1",
        "6.10.1-1~exp1",
        "6.10.11-1",
        "6.10.11-1~bpo12+1",
        "6.10.12-1",
        "6.10.3-1",
        "6.10.4-1",
        "6.10.6-1",
        "6.10.6-1~bpo12+1",
        "6.10.7-1",
        "6.10.9-1",
        "6.11-1~exp1",
        "6.11.10-1",
        "6.11.10-1~bpo12+1",
        "6.11.2-1",
        "6.11.4-1",
        "6.11.5-1",
        "6.11.5-1~bpo12+1",
        "6.11.6-1",
        "6.11.7-1",
        "6.11.9-1",
        "6.11~rc4-1~exp1",
        "6.11~rc5-1~exp1",
        "6.12.10-1",
        "6.12.11-1",
        "6.12.11-1+alpha",
        "6.12.11-1+alpha.1",
        "6.12.12-1",
        "6.12.12-1~bpo12+1",
        "6.12.13-1",
        "6.12.15-1",
        "6.12.16-1",
        "6.12.17-1",
        "6.12.19-1",
        "6.12.20-1",
        "6.12.21-1",
        "6.12.22-1",
        "6.12.22-1~bpo12+1",
        "6.12.25-1",
        "6.12.27-1",
        "6.12.27-1~bpo12+1",
        "6.12.29-1",
        "6.12.3-1",
        "6.12.30-1",
        "6.12.30-1~bpo12+1",
        "6.12.31-1",
        "6.12.32-1",
        "6.12.32-1~bpo12+1",
        "6.12.33-1",
        "6.12.35-1",
        "6.12.5-1",
        "6.12.6-1",
        "6.12.8-1",
        "6.12.9-1",
        "6.12.9-1+alpha",
        "6.12.9-1~bpo12+1",
        "6.12~rc6-1~exp1",
        "6.13.10-1~exp1",
        "6.13.11-1~exp1",
        "6.13.2-1~exp1",
        "6.13.3-1~exp1",
        "6.13.4-1~exp1",
        "6.13.5-1~exp1",
        "6.13.6-1~exp1",
        "6.13.7-1~exp1",
        "6.13.8-1~exp1",
        "6.13.9-1~exp1",
        "6.13~rc6-1~exp1",
        "6.13~rc7-1~exp1",
        "6.14.3-1~exp1",
        "6.14.5-1~exp1",
        "6.14.6-1~exp1",
        "6.15-1~exp1",
        "6.15.1-1~exp1",
        "6.15.2-1~exp1",
        "6.15.3-1~exp1",
        "6.15.4-1~exp1",
        "6.15.5-1~exp1",
        "6.15~rc7-1~exp1",
        "6.3.1-1~exp1",
        "6.3.11-1",
        "6.3.2-1~exp1",
        "6.3.4-1~exp1",
        "6.3.5-1~exp1",
        "6.3.7-1",
        "6.3.7-1~bpo12+1",
        "6.4.1-1~exp1",
        "6.4.11-1",
        "6.4.13-1",
        "6.4.4-1",
        "6.4.4-1~bpo12+1",
        "6.4.4-2",
        "6.4.4-3",
        "6.4.4-3~bpo12+1",
        "6.4~rc6-1~exp1",
        "6.4~rc7-1~exp1",
        "6.5.1-1~exp1",
        "6.5.10-1",
        "6.5.10-1~bpo12+1",
        "6.5.13-1",
        "6.5.3-1",
        "6.5.3-1~bpo12+1",
        "6.5.6-1",
        "6.5.8-1",
        "6.5~rc4-1~exp1",
        "6.5~rc6-1~exp1",
        "6.5~rc7-1~exp1",
        "6.6.11-1",
        "6.6.13-1",
        "6.6.13-1~bpo12+1",
        "6.6.15-1",
        "6.6.15-2",
        "6.6.3-1~exp1",
        "6.6.4-1~exp1",
        "6.6.7-1~exp1",
        "6.6.8-1",
        "6.6.9-1",
        "6.7-1~exp1",
        "6.7.1-1~exp1",
        "6.7.12-1",
        "6.7.12-1~bpo12+1",
        "6.7.4-1~exp1",
        "6.7.7-1",
        "6.7.9-1",
        "6.7.9-2",
        "6.8.11-1",
        "6.8.12-1",
        "6.8.12-1~bpo12+1",
        "6.8.9-1",
        "6.9.10-1",
        "6.9.10-1~bpo12+1",
        "6.9.11-1",
        "6.9.12-1",
        "6.9.2-1~exp1",
        "6.9.7-1",
        "6.9.7-1~bpo12+1",
        "6.9.8-1",
        "6.9.9-1"
      ]
    }
  ],
  "details": "In the Linux kernel, the following vulnerability has been resolved:\n\nmm: userfaultfd: fix race of userfaultfd_move and swap cache\n\nThis commit fixes two kinds of races, they may have different results:\n\nBarry reported a BUG_ON in commit c50f8e6053b0, we may see the same\nBUG_ON if the filemap lookup returned NULL and folio is added to swap\ncache after that.\n\nIf another kind of race is triggered (folio changed after lookup) we\nmay see RSS counter is corrupted:\n\n[  406.893936] BUG: Bad rss-counter state mm:ffff0000c5a9ddc0\ntype:MM_ANONPAGES val:-1\n[  406.894071] BUG: Bad rss-counter state mm:ffff0000c5a9ddc0\ntype:MM_SHMEMPAGES val:1\n\nBecause the folio is being accounted to the wrong VMA.\n\nI'm not sure if there will be any data corruption though, seems no. \nThe issues above are critical already.\n\n\nOn seeing a swap entry PTE, userfaultfd_move does a lockless swap cache\nlookup, and tries to move the found folio to the faulting vma.  Currently,\nit relies on checking the PTE value to ensure that the moved folio still\nbelongs to the src swap entry and that no new folio has been added to the\nswap cache, which turns out to be unreliable.\n\nWhile working and reviewing the swap table series with Barry, following\nexisting races are observed and reproduced [1]:\n\nIn the example below, move_pages_pte is moving src_pte to dst_pte, where\nsrc_pte is a swap entry PTE holding swap entry S1, and S1 is not in the\nswap cache:\n\nCPU1                               CPU2\nuserfaultfd_move\n  move_pages_pte()\n    entry = pte_to_swp_entry(orig_src_pte);\n    // Here it got entry = S1\n    ... < interrupted> ...\n                                   <swapin src_pte, alloc and use folio A>\n                                   // folio A is a new allocated folio\n                                   // and get installed into src_pte\n                                   <frees swap entry S1>\n                                   // src_pte now points to folio A, S1\n                                   // has swap count == 0, it can be freed\n                                   // by folio_swap_swap or swap\n                                   // allocator's reclaim.\n                                   <try to swap out another folio B>\n                                   // folio B is a folio in another VMA.\n                                   <put folio B to swap cache using S1 >\n                                   // S1 is freed, folio B can use it\n                                   // for swap out with no problem.\n                                   ...\n    folio = filemap_get_folio(S1)\n    // Got folio B here !!!\n    ... < interrupted again> ...\n                                   <swapin folio B and free S1>\n                                   // Now S1 is free to be used again.\n                                   <swapout src_pte & folio A using S1>\n                                   // Now src_pte is a swap entry PTE\n                                   // holding S1 again.\n    folio_trylock(folio)\n    move_swap_pte\n      double_pt_lock\n      is_pte_pages_stable\n      // Check passed because src_pte == S1\n      folio_move_anon_rmap(...)\n      // Moved invalid folio B here !!!\n\nThe race window is very short and requires multiple collisions of multiple\nrare events, so it's very unlikely to happen, but with a deliberately\nconstructed reproducer and increased time window, it can be reproduced\neasily.\n\nThis can be fixed by checking if the folio returned by filemap is the\nvalid swap cache folio after acquiring the folio lock.\n\nAnother similar race is possible: filemap_get_folio may return NULL, but\nfolio (A) could be swapped in and then swapped out again using the same\nswap entry after the lookup.  In such a case, folio (A) may remain in the\nswap cache, so it must be moved too:\n\nCPU1                               CPU2\nuserfaultfd_move\n  move_pages_pte()\n    entry = pte_to_swp_entry(orig_src_pte);\n    // Here it got entry = S1, and S1 is not in swap cache\n    folio = filemap_get\n---truncated---",
  "id": "CVE-2025-38242",
  "modified": "2025-07-10T15:10:49.187664Z",
  "published": "2025-07-09T11:15:26Z",
  "references": [
    {
      "type": "WEB",
      "url": "https://git.kernel.org/stable/c/0ea148a799198518d8ebab63ddd0bb6114a103bc"
    },
    {
      "type": "WEB",
      "url": "https://git.kernel.org/stable/c/db2ca8074955ca64187a4fb596dd290b9c446cd3"
    },
    {
      "type": "ADVISORY",
      "url": "https://security-tracker.debian.org/tracker/CVE-2025-38242"
    }
  ],
  "schema_version": "1.6.0"
}
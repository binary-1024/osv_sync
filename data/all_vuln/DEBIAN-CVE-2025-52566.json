{"schema_version":"1.7.3","id":"DEBIAN-CVE-2025-52566","published":"2025-06-24T04:15:46Z","modified":"2025-09-30T05:20:47.637638Z","withdrawn":"2025-11-04T08:06:40.824597Z","upstream":["CVE-2025-52566"],"details":"llama.cpp is an inference of several LLM models in C/C++. Prior to version b5721, there is a signed vs. unsigned integer overflow in llama.cpp's tokenizer implementation (llama_vocab::tokenize) (src/llama-vocab.cpp:3036) resulting in unintended behavior in tokens copying size comparison. Allowing heap-overflowing llama.cpp inferencing engine with carefully manipulated text input during tokenization process. This issue has been patched in version b5721.","affected":[{"package":{"name":"llama.cpp","ecosystem":"Debian:14","purl":"pkg:deb/debian/llama.cpp?arch=source"},"ranges":[{"type":"ECOSYSTEM","events":[{"introduced":"0"},{"fixed":"5760+dfsg-1"}]}],"versions":["5151+dfsg-1~exp2","5151+dfsg-1~exp3","5318+dfsg-1","5318+dfsg-2","5713+dfsg-1"],"ecosystem_specific":{"urgency":"not yet assigned"},"database_specific":{"source":"https://storage.googleapis.com/debian-osv/debian-cve-osv/DEBIAN-CVE-2025-52566.json"}}],"references":[{"type":"ADVISORY","url":"https://security-tracker.debian.org/tracker/CVE-2025-52566"}],"severity":[{"type":"CVSS_V3","score":"CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H"}]}
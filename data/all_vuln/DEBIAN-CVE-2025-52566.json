{
  "affected": [
    {
      "database_specific": {
        "source": "https://storage.googleapis.com/debian-osv/debian-cve-osv/DEBIAN-CVE-2025-52566.json"
      },
      "ecosystem_specific": {
        "urgency": "not yet assigned"
      },
      "package": {
        "ecosystem": "Debian:14",
        "name": "llama.cpp",
        "purl": "pkg:deb/debian/llama.cpp?arch=source"
      },
      "ranges": [
        {
          "events": [
            {
              "introduced": "0"
            },
            {
              "fixed": "5760+dfsg-1"
            }
          ],
          "type": "ECOSYSTEM"
        }
      ],
      "versions": [
        "5151+dfsg-1~exp2",
        "5151+dfsg-1~exp3",
        "5318+dfsg-1",
        "5318+dfsg-2",
        "5713+dfsg-1"
      ]
    }
  ],
  "details": "llama.cpp is an inference of several LLM models in C/C++. Prior to version b5721, there is a signed vs. unsigned integer overflow in llama.cpp's tokenizer implementation (llama_vocab::tokenize) (src/llama-vocab.cpp:3036) resulting in unintended behavior in tokens copying size comparison. Allowing heap-overflowing llama.cpp inferencing engine with carefully manipulated text input during tokenization process. This issue has been patched in version b5721.",
  "id": "DEBIAN-CVE-2025-52566",
  "modified": "2025-09-30T05:20:47.637638Z",
  "published": "2025-06-24T04:15:46Z",
  "references": [
    {
      "type": "ADVISORY",
      "url": "https://security-tracker.debian.org/tracker/CVE-2025-52566"
    }
  ],
  "schema_version": "1.7.3",
  "severity": [
    {
      "score": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H",
      "type": "CVSS_V3"
    }
  ],
  "upstream": [
    "CVE-2025-52566"
  ]
}
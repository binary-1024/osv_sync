{"schema_version":"1.7.3","id":"UBUNTU-CVE-2025-49847","published":"2025-06-17T20:15:00Z","modified":"2025-10-10T18:02:27.838634Z","upstream":["CVE-2025-49847"],"details":"llama.cpp is an inference of several LLM models in C/C++. Prior to version b5662, an attacker‐supplied GGUF model vocabulary can trigger a buffer overflow in llama.cpp’s vocabulary‐loading code. Specifically, the helper _try_copy in llama.cpp/src/vocab.cpp: llama_vocab::impl::token_to_piece() casts a very large size_t token length into an int32_t, causing the length check (if (length < (int32_t)size)) to be bypassed. As a result, memcpy is still called with that oversized size, letting a malicious model overwrite memory beyond the intended buffer. This can lead to arbitrary memory corruption and potential code execution. This issue has been patched in version b5662.","affected":[{"package":{"name":"llama.cpp","ecosystem":"Ubuntu:25.10","purl":"pkg:deb/ubuntu/llama.cpp@5882+dfsg-2?arch=source&distro=questing"},"ranges":[{"type":"ECOSYSTEM","events":[{"introduced":"0"}]}],"versions":["5318+dfsg-1","5318+dfsg-2","5713+dfsg-1","5760+dfsg-1","5760+dfsg-3","5760+dfsg-4","5882+dfsg-2"],"ecosystem_specific":{"binaries":[{"binary_name":"llama.cpp","binary_version":"5882+dfsg-2"}]},"database_specific":{"source":"https://github.com/canonical/ubuntu-security-notices/blob/main/osv/cve/2025/UBUNTU-CVE-2025-49847.json"}}],"references":[{"type":"REPORT","url":"https://ubuntu.com/security/CVE-2025-49847"},{"type":"REPORT","url":"https://www.cve.org/CVERecord?id=CVE-2025-49847"},{"type":"REPORT","url":"https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8wwf-w4qm-gpqr"},{"type":"REPORT","url":"https://github.com/ggml-org/llama.cpp/commit/3cfbbdb44e08fd19429fed6cc85b982a91f0efd5"}],"severity":[{"type":"CVSS_V3","score":"CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H"},{"type":"Ubuntu","score":"medium"}]}